# Resume Screening ML Pipeline

This project is an end-to-end machine learning pipeline for automated resume screening. It uses synthetic data generated by GPT to match resumes with job descriptions and predict if a candidate is a good fit for a role.

## Features
- Synthetic data generation using GPT (resumes and job descriptions)
- Advanced NLP preprocessing and feature engineering
  - Multiple similarity metrics (cosine, euclidean, manhattan, common terms)
  - TF-IDF vectorization with optimized parameters
- Model training with hyperparameter tuning
- Comprehensive evaluation metrics
- REST API for predictions with confidence scores
- Containerized with Docker
- Complete test suite with realistic test cases
- GitHub Actions CI/CD pipeline for automated testing and deployment

## Directory Structure
```
resume_screening_ml/
├── data/                # Generated datasets and trained models
├── notebooks/           # Jupyter notebooks for EDA and prototyping
├── src/                 # Source code
│   ├── __init__.py
│   ├── data_generation.py  # GPT-based synthetic data generation
│   ├── preprocessing.py    # Text cleaning and feature extraction
│   ├── train.py           # Model training with hyperparameter tuning
│   └── predict_api.py     # FastAPI service for predictions
├── tests/               # Unit and integration tests
│   ├── __init__.py
│   ├── test_preprocessing.py  # Tests for feature extraction
│   ├── test_predictions.py    # Tests with sample resumes/jobs
│   └── test_cases.json        # Sample test cases
├── requirements.txt     # Python dependencies
├── Dockerfile           # For containerization
├── .env.example         # Template for environment variables
├── .gitignore           # Ignore patterns
└── README.md            # Project overview
```

## Getting Started
1. Clone the repository
2. Create and activate a virtual environment: 
   ```
   python -m venv .venv
   source .venv/bin/activate  # On Windows: .venv\Scripts\activate
   ```
3. Install dependencies: `pip install -r requirements.txt`
4. Create a `.env` file from `.env.example` and add your OpenAI API key
5. Generate synthetic data: `python src/data_generation.py`
   - A sample data file is included at `data/sample_resume_job_dataset.json` for reference
6. Preprocess and extract features: `python src/preprocessing.py`
7. Train the model: `python src/train.py`
8. Run the API: `python src/predict_api.py`
9. Access the API documentation at http://localhost:8000/docs

## Requirements
- Python 3.8+
- OpenAI API key (for GPT-based data generation)

## Testing
The project includes a comprehensive test suite:

```bash
# Run all tests
python -m pytest

# Run specific test files with verbose output
python -m pytest tests/test_preprocessing.py -v
python -m pytest tests/test_predictions.py -v
```

## Model Features
The model uses multiple similarity metrics to compare resumes and job descriptions:

1. **Cosine Similarity**: Measures the cosine of the angle between text vectors
2. **Euclidean Distance**: Measures the straight-line distance between text vectors
3. **Manhattan Distance**: Measures the sum of absolute differences between vector dimensions
4. **Common Terms Ratio**: Measures the proportion of terms that appear in both texts

## API Endpoints

- `GET /`: Root endpoint with API information
- `GET /health`: Health check endpoint
- `POST /predict`: Prediction endpoint that accepts resume and job description

## CI/CD Pipeline
This project includes a GitHub Actions workflow (`.github/workflows/main.yml`) that automates testing and deployment:

- **Continuous Integration**: Automatically runs tests on every push and pull request
- **Automated Build**: Builds Docker image when tests pass on the main branch
- **Deployment Ready**: Includes a commented deployment job that can be customized for your hosting environment

To use the CI/CD pipeline:
1. Add your `OPENAI_API_KEY` to GitHub repository secrets
2. Push your code to GitHub
3. Uncomment and customize the deployment section when ready to deploy

## Future Enhancements
- Add web UI for easier interaction
- Implement more sophisticated features (skill extraction, keyword matching)
- Deploy to cloud provider (AWS, GCP, Azure, DO)
- Add user authentication and rate limiting

---
